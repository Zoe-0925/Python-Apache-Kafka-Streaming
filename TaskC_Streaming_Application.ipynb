{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import Geohash as gh\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "def sendDataToDB(iter):\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    isfire = db.isfire\n",
    "    cli = db.cli\n",
    "    \n",
    "    datas = []\n",
    "   \n",
    "    for record in iter:\n",
    "        temp = json.loads(record[1])\n",
    "        temp['geo'] = gh.encode(temp['latitude'],temp['longitude'],precision=5)   \n",
    "        datas.append(temp)  \n",
    "        \n",
    "        if temp['sender_id'] == 'producer1':\n",
    "            try:\n",
    "                cli.insert_one(temp)\n",
    "            except Exception as ex:\n",
    "                print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "    \n",
    "    print('no.of datas' + str(len(datas)))\n",
    "    \n",
    "    if len(datas)== 3:\n",
    "        pro1 = []\n",
    "        pro2 = []\n",
    "        for x in datas:\n",
    "            if x['sender_id'] == 'producer1':\n",
    "                pro1.append(x)\n",
    "            else:\n",
    "                pro2.append(x)\n",
    "        \n",
    "        for i in range(len(pro1)):\n",
    "            if pro1[i]['geo'] == pro2[0]['geo']:\n",
    "                try:\n",
    "                    isfire.insert_one(pro2[0])\n",
    "                    break\n",
    "                except Exception as ex:\n",
    "                    print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "           \n",
    "    if len(datas)== 4:\n",
    "        pro1 = []\n",
    "        pro2 = []\n",
    "        \n",
    "        for x in datas:\n",
    "            if x['sender_id'] == 'producer1':\n",
    "                pro1.append(x)\n",
    "            else:\n",
    "                pro2.append(x)\n",
    "        \n",
    "\n",
    "        if len(pro2)== 2:\n",
    "            if pro2[0]['geo'] == pro2[1]['geo']:\n",
    "                avr_surface = ((int)(pro2[0]['surface_temperature_celcius']) + (int)(pro2[1]['surface_temperature_celcius']))/2\n",
    "                avr_conf = ((int)(pro2[0]['confidence']) + (int)(pro2[1]['confidence']))/2\n",
    "                \n",
    "                pro2[0]['surface_temperature_celcius'] = avr_surface\n",
    "                pro2[0]['confidence'] = avr_conf\n",
    "                \n",
    "                for y in pro1:\n",
    "                    if pro2[0]['geo'] == y['geo']:\n",
    "                        try:\n",
    "                            isfire.insert_one(pro2[0])\n",
    "                            break\n",
    "                        except Exception as ex:\n",
    "                            print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "            \n",
    "            else:\n",
    "                counter = 0\n",
    "                \n",
    "                if counter < 2:\n",
    "                    for entry1 in pro1:\n",
    "                        for entry2 in pro2:\n",
    "                            if entry1['geo'] == entry2['geo']:\n",
    "                                try:\n",
    "                                    isfire.insert_one(entry2)\n",
    "                                    counter = counter + 1\n",
    "                                except Exception as ex:\n",
    "                                    print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "\n",
    "                        \n",
    "                    \n",
    "    \n",
    "    client.close()\n",
    "\n",
    "n_secs = 10\n",
    "topic = \"fire\"\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week12-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
